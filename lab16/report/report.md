---
## Front matter
title: "Отчёт по лабораторной работе №16"
subtitle: "Программный RAID"
author: "Ришард Когенгар"

## Generic otions
lang: ru-RU
toc-title: "Содержание"

## Bibliography
bibliography: bib/cite.bib
csl: pandoc/csl/gost-r-7-0-5-2008-numeric.csl

## Pdf output format
toc: true
toc-depth: 2
lof: true
lot: true
fontsize: 12pt
linestretch: 1.5
papersize: a4
documentclass: scrreprt
## I18n polyglossia
polyglossia-lang:
  name: russian
  options:
    - spelling=modern
    - babelshorthands=true
polyglossia-otherlangs:
  name: english
## I18n babel
babel-lang: russian
babel-otherlangs: english
## Fonts
mainfont: IBM Plex Serif
romanfont: IBM Plex Serif
sansfont: IBM Plex Sans
monofont: IBM Plex Mono
mathfont: STIX Two Math
mainfontoptions: Ligatures=Common,Ligatures=TeX,Scale=0.94
romanfontoptions: Ligatures=Common,Ligatures=TeX,Scale=0.94
sansfontoptions: Ligatures=Common,Ligatures=TeX,Scale=MatchLowercase,Scale=0.94
monofontoptions: Scale=MatchLowercase,Scale=0.94,FakeStretch=0.9
mathfontoptions:
## Biblatex
biblatex: true
biblio-style: "gost-numeric"
biblatexoptions:
  - parentracker=true
  - backend=biber
  - hyperref=auto
  - language=auto
  - autolang=other*
  - citestyle=gost-numeric
## Pandoc-crossref LaTeX customization
figureTitle: "Рис."
tableTitle: "Таблица"
listingTitle: "Листинг"
lofTitle: "Список иллюстраций"
lotTitle: "Список таблиц"
lolTitle: "Листинги"
## Misc options
indent: true
header-includes:
  - \usepackage{indentfirst}
  - \usepackage{float}
  - \floatplacement{figure}{H}
---

# Цель работы

Освоить работу с RAID-массивами при помощи утилиты mdadm.

# Ход выполнения

## Создание виртуальных носителей

1. После завершения установки операционной системы выполнен переход к настройкам виртуальной машины в среде виртуализации **Oracle VirtualBox**.  
   В разделе **«Носители»** открыт контроллер **SATA**, к которому ранее был подключён основной виртуальный жёсткий диск.

2. С использованием встроенного менеджера виртуальных носителей последовательно созданы **три дополнительных виртуальных жёстких диска**.  
   Для каждого диска выбран формат **VDI (VirtualBox Disk Image)** и тип распределения пространства **обычный (динамический)**.  
   Размер каждого создаваемого диска установлен равным **512 MiB**, что соответствует требованиям задания.

   ![Подключение дополнительных виртуальных дисков к контроллеру SATA](Screenshot_1.png)

## Создание RAID-диска

1. Виртуальная машина была запущена, после чего выполнено получение полномочий администратора командой `su -`.  
   Дальнейшие операции выполнялись от имени пользователя **root**.

2. Для проверки наличия ранее добавленных виртуальных дисков выполнен вывод списка накопителей командой `fdisk -l | grep /dev/sd`.  
   В системе корректно определились дополнительные диски объёмом **512 MiB**, отображаемые как **/dev/sdd**, **/dev/sde**, **/dev/sdf**, что соответствует требованиям задания.

   ![Проверка наличия добавленных дисков /dev/sdd, /dev/sde, /dev/sdf](Screenshot_2.png)

3. На каждом из трёх дисков создан раздел при помощи утилиты `sfdisk`.  
   В результате на диске **/dev/sdd** сформирован раздел **/dev/sdd1** (аналогичные действия выполнены для **/dev/sde** и **/dev/sdf**).

   ![Создание раздела на диске /dev/sdd с помощью sfdisk](Screenshot_3.png)

4. Выполнена проверка текущего типа созданных разделов командами `sfdisk --print-id /dev/sdX 1`.  
   Установлено, что созданные разделы имеют тип **83 (Linux)**, что соответствует стандартному типу раздела Linux по умолчанию.

5. Для определения доступных типов разделов, относящихся к RAID, выполнен просмотр списка типов командой `sfdisk -T | grep -i raid`.  
   В перечне отображается тип **Linux raid autodetect**, который используется для разметки RAID-разделов в рамках данной лабораторной работы.

6. Тип каждого из созданных разделов был изменён на **Linux raid autodetect** с идентификатором **fd** командами `sfdisk --change-id /dev/sdX 1 fd`.  
   Таким образом, разделы **/dev/sdd1**, **/dev/sde1**, **/dev/sdf1** были подготовлены для включения в RAID-массив.

   ![Проверка типа разделов и изменение на Linux raid autodetect (fd)](Screenshot_4.png)

7. Выполнена проверка текущего состояния дисков и созданных разделов командами `sfdisk -l /dev/sdd`, `sfdisk -l /dev/sde`, `sfdisk -l /dev/sdf`.  
   Установлено, что на каждом диске создан один раздел объёмом порядка **511 MiB**, а тип раздела установлен как **fd Linux raid autodetect**.  
   Это подтверждает корректную подготовку всех трёх дисков для дальнейшей сборки RAID.

   ![Состояние дисков после изменения типа разделов на fd](Screenshot_5.png)

8. Для сборки программного RAID использована утилита **mdadm** (при необходимости должна быть предварительно установлена в системе).  

9. С использованием `mdadm` создан массив **RAID 1** из двух дисков: **/dev/sdd1** и **/dev/sde1**.  
   В результате массив был создан как устройство **/dev/md0**.

10. Состояние массива проверено командами `cat /proc/mdstat`, `mdadm --query /dev/md0`, `mdadm --detail /dev/md0`.  
   По данным `/proc/mdstat` массив **md0** активен, уровень RAID — **raid1**, количество устройств — **2**, состояние синхронизации корректное (**[UU]**).  
   Вывод `mdadm --query` подтверждает тип массива RAID1 и наличие двух устройств без резервных (spares).

   ![Создание RAID1 и проверка состояния через /proc/mdstat и mdadm --query](Screenshot_6.png)

11. Детальная информация о массиве получена командой `mdadm --detail /dev/md0`.  
   Массив находится в состоянии **clean**, активны два устройства (**/dev/sdd1** и **/dev/sde1**) со статусом **active sync**, ошибок и отказавших устройств не обнаружено.

   ![Детальная информация о массиве md0](Screenshot_7.png)

12. На созданном RAID-массиве **/dev/md0** сформирована файловая система **ext4** командой `mkfs.ext4 /dev/md0`.

13. Выполнено монтирование массива в файловую систему.  
   При попытке создать каталог `/data` система сообщила, что каталог уже существует, поэтому для монтирования был создан отдельный каталог **/mnt/raid**, после чего выполнено подключение массива командой `mount /dev/md0 /mnt/raid`.

   ![Создание файловой системы ext4 и монтирование массива](Screenshot_8.png)

14. Для обеспечения автомонтирования массива после перезагрузки в файл **/etc/fstab** добавлена запись для **/dev/md0** с точкой монтирования **/mnt/raid**, типом файловой системы **ext4** и параметрами `defaults 1 2`.

   ![Добавление записи для автомонтирования в /etc/fstab](Screenshot_9.png)

15. Сымитирован отказ одного из дисков массива командой `mdadm /dev/md0 --fail /dev/sde1`.  
   После этого сбойный диск удалён из массива командой `mdadm /dev/md0 --remove /dev/sde1`.

16. Для восстановления зеркала RAID 1 выполнена замена диска: в массив добавлен третий подготовленный раздел **/dev/sdf1** командой `mdadm /dev/md0 --add /dev/sdf1`.  
   По итогам проверки командой `mdadm --detail /dev/md0` активными устройствами массива являются **/dev/sdd1** и **/dev/sdf1**, массив находится в состоянии **clean**, отказавшие устройства отсутствуют.

   ![Имитация отказа, удаление диска и добавление нового устройства в RAID](Screenshot_10.png)

17. После завершения проверки работоспособности массива выполнено удаление RAID и очистка метаданных:  
   массив был размонтирован, остановлен командой `mdadm --stop /dev/md0`, затем на всех трёх разделах выполнена очистка служебной информации RAID командой `mdadm --zero-superblock`.

   ![Остановка массива и очистка RAID-метаданных](Screenshot_11.png)

## RAID-массив с горячим резервом (hotspare)

1. После запуска виртуальной машины выполнено получение полномочий администратора командой `su -`.  
   Все дальнейшие операции проводились от имени пользователя **root**.

2. С использованием утилиты **mdadm** создан массив **RAID 1** из двух разделов **/dev/sdd1** и **/dev/sde1**.  
   В качестве устройства массива назначено **/dev/md0**. При создании массива подтверждено продолжение операции, после чего массив был успешно запущен.

3. В созданный массив добавлен третий раздел **/dev/sdf1** в качестве горячего резерва (spare) командой `mdadm --add /dev/md0 /dev/sdf1`.  
   Таким образом, массив содержит два активных устройства и одно резервное.

4. Выполнена попытка монтирования массива командой `mount /dev/md0`.  
   Система дополнительно вывела уведомление о том, что файл `/etc/fstab` был изменён и для применения изменений рекомендуется выполнить перечитывание конфигурации через `systemctl daemon-reload`.

5. Для контроля состояния массива выполнены команды `cat /proc/mdstat`, `mdadm --query /dev/md0`, `mdadm --detail /dev/md0`.  
   По выводу `/proc/mdstat` массив **md0** активен и работает в режиме **raid1**, состояние массива корректное (**[UU]**).  
   Вывод `mdadm --query` показывает наличие **2 устройств** и **1 spare**, что подтверждает подключение горячего резерва.  
   В выводе `mdadm --detail` зафиксировано, что **Total Devices = 3**, при этом **Active Devices = 2**, а **/dev/sdf1** имеет статус **spare**.

   ![Создание RAID1, добавление hotspare и проверка состояния /proc/mdstat и mdadm --query](Screenshot_12.png)

6. Детальная информация о составе массива подтверждает наличие горячего резерва:  
   активными устройствами являются **/dev/sdd1** и **/dev/sde1** (статус **active sync**), а **/dev/sdf1** отображается как **spare**.  
   Массив находится в состоянии **clean**, отказавшие устройства отсутствуют.

   ![Состояние массива md0: два active sync и один spare](Screenshot_13.png)

7. Для имитации отказа одного из дисков выполнена команда `mdadm /dev/md0 --fail /dev/sde1`.  
   После имитации сбоя выполнена проверка командой `mdadm --detail /dev/md0`.

8. По результатам проверки установлено, что массив продолжает работать, а резервное устройство автоматически заменило отказавшее:  
   **/dev/sdf1** перешёл в состояние **active sync**, а **/dev/sde1** отображается как **faulty**.  
   В выводе также зафиксировано наличие **Failed Devices = 1**, при этом состояние массива остаётся **clean**, что свидетельствует об автоматической пересборке и сохранении работоспособности RAID 1.

   ![Имитация отказа диска и автоматическая замена hotspare](Screenshot_14.png)

9. После завершения проверки массив был остановлен и метаданные RAID очищены.  
   Выполнено размонтирование (`umount /dev/md0`), остановка массива (`mdadm --stop /dev/md0`), после чего очищены суперблоки RAID на разделах `mdadm --zero-superblock` для **/dev/sdd1**, **/dev/sde1**, **/dev/sdf1**.

## Преобразование массива RAID 1 в RAID 5

1. Выполнено получение полномочий администратора командой `su -`.

2. Создан массив **RAID 1** из двух дисков **/dev/sdd1** и **/dev/sde1** с устройством массива **/dev/md0**.

3. В массив добавлен третий диск **/dev/sdf1** командой `mdadm --add /dev/md0 /dev/sdf1`.  
   После добавления устройство было доступно как резервное (spare), что подтверждалось выводом `mdadm --detail`.

4. Для контроля состояния массива выполнены команды `cat /proc/mdstat`, `mdadm --query /dev/md0`, `mdadm --detail /dev/md0`.  
   На данном этапе массив работал как **raid1**, при этом третий диск присутствовал в составе как **spare**.

   ![Проверка состояния массива RAID1 перед преобразованием](Screenshot_16.png)

5. Выполнено изменение уровня массива командой `mdadm --grow /dev/md0 --level=5`.  
   По результатам команды уровень массива был изменён на **raid5**, при этом в детальном выводе `mdadm --detail` фиксируются параметры RAID 5 (в том числе **Layout: left-symmetric** и **Chunk Size: 64K**).  
   На данном этапе третий диск ещё отображается как **spare**, а активных устройств остаётся два.

   ![Изменение уровня массива на RAID5 и проверка mdadm --detail](Screenshot_17.png)

6. Для полноценного перехода к RAID 5 выполнено изменение числа дисков в массиве до трёх командой `mdadm --grow /dev/md0 --raid-devices=3`.  
   После выполнения операции массив использует все три устройства как активные.

7. По данным `mdadm --detail /dev/md0` массив находится в состоянии **clean**, количество устройств **Raid Devices = 3**, активных устройств **Active Devices = 3**, резервные отсутствуют (**Spare Devices = 0**).  
   Размер массива увеличился до **1020 MiB**, что соответствует режиму RAID 5 с тремя дисками по 512 MiB.

   ![RAID5 после увеличения числа устройств до 3: все диски active sync](Screenshot_18.png)

8. После завершения проверки массив был удалён, а метаданные очищены:  
   выполнено размонтирование (`umount /dev/md0`), остановка массива (`mdadm --stop /dev/md0`), очистка суперблоков `mdadm --zero-superblock` для **/dev/sdd1**, **/dev/sde1**, **/dev/sdf1**.  
   При наличии записи автомонтирования в `/etc/fstab` выполнено её отключение (комментирование), чтобы исключить ошибки монтирования при последующих перезагрузках.


## Вывод

В ходе выполнения лабораторной работы были освоены практические приёмы создания и администрирования программных RAID-массивов в операционной системе Linux с использованием утилиты **mdadm**.  
Были успешно созданы RAID-массивы уровней **RAID 1** и **RAID 5**, выполнена их инициализация, проверка состояния и монтирование в файловую систему.

# Контрольные вопросы

**1. Приведите определение RAID**

RAID (Redundant Array of Independent Disks) — это технология объединения нескольких физических жёстких дисков в единый логический массив с целью повышения отказоустойчивости, производительности и/или эффективности использования дискового пространства.  
RAID реализуется как аппаратно (с использованием RAID-контроллеров), так и программно (средствами операционной системы, например утилитой `mdadm` в Linux).

**2. Какие типы RAID-массивов существуют на сегодняшний день**

На практике используются следующие основные уровни RAID:

- **RAID 0** — чередование данных (striping), без избыточности  
- **RAID 1** — зеркалирование (mirroring)  
- **RAID 5** — чередование с распределённой чётностью  
- **RAID 6** — чередование с двойной распределённой чётностью  
- **RAID 10 (1+0)** — комбинация RAID 1 и RAID 0  

Кроме классических уровней существуют также вложенные и специализированные реализации (RAID 50, RAID 60 и др.), применяемые в корпоративных системах хранения данных.

**3. Охарактеризуйте RAID 0, RAID 1, RAID 5, RAID 6: алгоритм работы, назначение, примеры применения**

**RAID 0**  
Алгоритм работы основан на чередовании блоков данных между несколькими дисками. Избыточность отсутствует, данные распределяются по всем накопителям.  
Назначение — повышение производительности операций чтения и записи.  
Отказоустойчивость отсутствует: выход из строя одного диска приводит к потере всех данных.  
Применение: временные хранилища, рабочие каталоги с высокой нагрузкой, системы обработки видео и графики, где важна скорость, но не критична сохранность данных.

**RAID 1**  
Алгоритм основан на зеркалировании: все данные полностью дублируются на каждом диске массива.  
Назначение — обеспечение высокой надёжности и доступности данных.  
Массив сохраняет работоспособность при отказе одного из дисков.  
Применение: системные разделы, серверы баз данных, файловые серверы, критически важные системы.

**RAID 5**  
Алгоритм работы использует чередование данных с распределённой информацией чётности, позволяющей восстановить данные при отказе одного диска.  
Назначение — баланс между отказоустойчивостью, производительностью и эффективным использованием дискового пространства.  
Массив допускает отказ одного диска без потери данных.  
Применение: файловые серверы, серверы приложений, корпоративные системы хранения.

**RAID 6**  
Алгоритм аналогичен RAID 5, но использует двойную распределённую чётность.  
Назначение — повышение надёжности по сравнению с RAID 5.  
Массив способен выдержать одновременный отказ двух дисков.  
Применение: крупные хранилища данных, системы с большим количеством накопителей, критически важные серверы, где высок риск одновременных отказов.
